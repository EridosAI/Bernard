---
date: 2026-01-22
tags:
  - jepa
  - memory
  - forgetting
  - ema
  - training
  - audio
  - semantic-space
---

# Ideas — 2026-01-22

## EMA Drift as Natural Forgetting

Concept from [[Dual JEPA Architecture|JEPA architecture]]: The target encoder updates via exponential moving average ([[EMA Drift|EMA]]) of the context encoder. This creates a moving target — the representation space drifts over time.

Applied to memory: Memories encoded six months ago were encoded in a different representation space than today's. As the encoder drifts:

- Recent memories stay "in sync" with current representations → easy retrieval
- Old memories become progressively misaligned → harder retrieval
- Time bias emerges naturally, no explicit decay timers needed

**Rehearsal as re-encoding:** Accessing a memory re-encodes it in the current space. This explains why frequently accessed memories stay vivid, and why memories subtly change over time — each recall rewrites them.

**Open concern:** Pure EMA drift would eventually lose all unaccessed memories, including significant ones that should persist. [[Intensity Weighting]] needs to counteract this somehow:

- Option A: High-intensity memories get automatic re-encoding during consolidation/dreaming
- Option B: Dual storage — stable anchor for intense memories alongside drifting representation
- Option C: Strong associations cause incidental re-encoding (intensity works through existing association mechanism)

**Status:** Interesting but needs more thought on intensity preservation before committing to implementation.

---

## Progressive JEPA Training

**Core insight:** Instead of training JEPA intensively with massive compute (the Meta approach), train progressively over time with modest compute. Experience accumulates → training data accumulates → model matures.

**What stays fixed:**
- Predictor architecture (attention layers, projection dimensions)
- Basic training machinery

**What grows:**
- [[Long-Term Memory]] embedding store (more objects, more memories)
- Predictor weights (refined through nightly consolidation)
- Training data (more co-occurrence pairs from more episodes)

**Developmental trajectory:**
- Early: Sparse associations, weak predictions, frequent "I don't have a guess"
- Middle: Denser association graph, better retrieval, emerging causal patterns
- Mature: Rich associative structure, handles most attribution internally

Mirrors biological development: A child's brain architecture doesn't radically change — the learned structure within it becomes more sophisticated. Same hardware, more refined patterns.

**Scaling trigger:** If training loss plateaus despite new data, the predictor has hit capacity. That's when you increase model size and retrain. But start small, let it grow organically, scale when you hit the ceiling.

**Implication:** The system isn't "trained" in a traditional sense — it's raised. Development time is measured in weeks and months of lived experience, not GPU hours.

---

## Semantic Audio JEPA (Future Direction)

**Core concept:** Replace the speech-to-text → LLM → text-to-speech pipeline with [[Audio JEPA]] that operates in [[Semantic Space]] throughout. No text bottleneck.

**Current paradigm:**
```
Sound → Whisper → Text → LLM → Text → TTS → Sound
```

**Proposed paradigm:**
```
Raw frequencies → Audio JEPA (learned phoneme/word structure) → Semantic space ← Visual JEPA
                                                             ↓
                                          Output as learned sound patterns
```

**Key insight:** Phonemes are to language what visual patches are to images — base units for JEPA to learn structure over. Sound patterns associate directly with meaning, not with text tokens.

**Pre-train for competence, not for text:**
- Train audio JEPA on speech data until it can parse and produce language patterns
- Internal representation stays semantic, not symbolic
- Enough to communicate out of the box — no one wants to train a baby assistant

**Experiential learning adds:**
- Grounding to visual/physical reality (sound pattern → object)
- Workshop-specific vocabulary learned naturally
- User's accent, speech patterns, emphasis habits
- Emotional register tied to actual contexts

**Emergent properties:**
- Accents emerge from learning actual acoustic patterns
- Prosody, emphasis, emotion preserved as features (not stripped by text conversion)
- Can learn concepts hard to verbalize but easy to demonstrate (motor running rough, hesitant vs confident "yes")
- Fast versatile learning: new concept = new grounding, no tokenization bottleneck

**The profound bit:** Current LLMs can only understand what's expressible in text. They've never *heard* frustration. A semantic audio system learns from actual acoustic-emotional patterns.

**Cross-modal integration:**
- [[Outward JEPA|Outward visual JEPA]]: structure of what you see
- Outward auditory JEPA: structure of what you hear
- [[Inward JEPA]]: associations across both modalities plus memory

**Roadmap position:**
- Current: Whisper pipeline (works now)
- Near-term: Build [[Associative Memory]] system
- Future: Swap Whisper for audio JEPA trained on workshop's sound environment

**Status:** Architecturally compatible with [[Dual JEPA Architecture|dual JEPA]] vision. Significant scope — park for later, but preserve the direction.

---

## Loose Threads

**Future addition — [[Intensity Weighting]]:** Emotion/significance as force multiplier on association strength. Explains why low-intensity repeated experiences fade while single high-intensity experiences persist for decades. Not needed for v1, but architecturally important later.

**Open questions:**
- Concrete implementation of [[Inward JEPA]] predictor
- How to determine "task demands memory" (learned threshold vs explicit signals)
- Update cadence: every episode or periodic consolidation ([[Dream Training|dreaming phase]])

**Related half-formed idea (parked for now):** Memory as past revisions of JEPA weights — model state *is* memory rather than separate storage. Needs more thought on how to access specific past states.

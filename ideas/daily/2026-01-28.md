---
date: 2026-01-28
tags:
  - vision
  - reading
  - text-recognition
  - jepa
  - semantic-space
---

# Ideas — 2026-01-28

## Reading as Visual Skill (Text Without OCR)

**Core concept:** Text is just another visual pattern. The same visual encoder that recognizes a hammer should recognize the word "hammer" — and both should land in the same [[Semantic Space|semantic region]]. No OCR pipeline. No text tokenization. Visual text patterns map directly to meaning.

**How humans read:**
```
[early reader]   see "hammer" → h-a-m-m-e-r → /hæmər/ → meaning
[fluent reader]  see "hammer" → meaning (direct)
```

Fluent readers don't decode letter-by-letter. The visual pattern of the word becomes fused with its meaning through experience. Bernard should learn the same way.

**The unified vision:**
```
                        SEMANTIC SPACE

        [see hammer]  ───────►  ◉ hammer region
        [hear "hammer"] ─────►  ◉
        [hear hammer sound] ──► ◉
        [see "hammer" written]► ◉  ← THIS IS READING
```

**Training approach — synthetic distillation:**

```python
# Generate training data
words = ["hammer", "drill", "wire", "capacitor", ...]

for word in words:
    # Create images of the word (vary font, size, rotation, background)
    text_images = render_word_variations(word, n=100)

    # Get semantic target from ImageBind
    target_embedding = imagebind.encode_text(word)  # 1024-dim

    # Training pairs
    for img in text_images:
        visual_embedding = vjepa.encode(img)
        projected = visual_projection(visual_embedding)
        loss = mse(projected, target_embedding)
```

**Variations to include:**
- Different fonts (serif, sans-serif, handwritten, stencil)
- Different sizes and aspect ratios
- Different rotations/perspectives
- Different backgrounds (paper, labels, screens, tool surfaces)
- Noise, blur, partial occlusion
- ALL CAPS, lowercase, Mixed Case

**Training data scale:**
- 10,000 words × 50 variations = 500,000 training images
- Words from: common vocabulary + workshop terms + user's specific labels
- Synthetic generation is cheap — render on-the-fly or pre-generate

**Phase 2 — Real-world text:**
Once synthetic training works, fine-tune on:
- Photos of labels in the workshop
- Text on tool packaging and equipment
- Handwritten notes
- Screenshots and displays

**What existing models do (and don't do):**

| Model | What it does | Why it's not enough |
|---|---|---|
| CLIP/SigLIP | Images with text influence embeddings | Weak, not trained specifically for reading |
| TrOCR | Image → text | Outputs tokens, not semantic embeddings |
| PaddleOCR | Image → text | Same — text bottleneck |
| Florence-2 | Can do OCR | Outputs text, not embeddings |

None produce: visual text pattern → semantic embedding directly.

**Relationship to Florence removal:**
This is part of treating Florence-2 as scaffolding. Currently:
- Florence detects objects and provides labels
- If text appears, would need separate OCR

Post-scaffolding:
- [[Dual JEPA Architecture|V-JEPA]] recognizes objects directly (no labels needed)
- V-JEPA recognizes text directly (no OCR needed)
- Both map to unified [[Semantic Space]]

**The deeper principle:** Text isn't special. It's a visual convention humans invented ~5000 years ago. A truly unified perceptual system learns to interpret text visually, the same way it interprets faces, tools, or any other meaningful pattern.

The label on your oscilloscope shouldn't need OCR. Bernard should learn that visual pattern means "oscilloscope" the same way it learns the device's shape means oscilloscope.

**Implementation priority:** Medium-low. Current scaffolding works. But this is the clean path when removing the text bottleneck entirely.

**Roadmap position:**
- Current: OCR if needed (scaffolding)
- Near-term: Synthetic text → semantic distillation
- Future: Real-world text fine-tuning
- End state: Written text is just another visual feature
